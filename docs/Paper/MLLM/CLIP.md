# CLIP

## Introduction

CLIP（Contrastive Language-Image Pre-Training）模型是一种多模态预训练神经网络，由OpenAI在2021年发布，是从自然语言监督中学习的一种有效且可扩展的方法。CLIP在预训练期间学习执行广泛的任务，包括OCR，地理定位，动作识别，并且在计算效率更高的同时优于公开可用的最佳ImageNet模型。
该模型的核心思想是使用大量图像和文本的配对数据进行预训练，以学习图像和文本之间的对齐关系。CLIP模型有两个模态，一个是文本模态，一个是视觉模态，包括两个主要部分：

1. Text Encoder：用于将文本转换为低维向量表示-Embeding。

2. Image Encoder：用于将图像转换为类似的向量表示-Embedding。

  

  ![image-20250223202917810](https://blog-pic-thorin.oss-cn-hangzhou.aliyuncs.com/image-20250223202917810.png)

## Train

CLIP预训练图像编码器和文本编码器，以预测数据集中哪些图像与哪些文本配对。然后，使用这种行为将CLIP转换为zero-shot分类器。将数据集的所有类转换为文本，例如“一张狗的照片”，并预测CLIP估计的标题类与给定图像的最佳配对。

模型中使用visual_embedding 叉乘 text_embedding，得到一个[N, N]的矩阵，那么对角线上的值便是成对特征内积得到的，如果visual_embedding和对应的text_embedding越相似，那么它的值便越大。

选取[N, N]矩阵中的第一行，代表第1个图片与N个文本的相似程度，其中第1个文本是正样本，将这一行的标签设置为1，那么就可以使用交叉熵进行训练，尽量把第1个图片和第一个文本的内积变得更大，那么它们就越相似。


$$ H(P, Q) = -\sum_{x} P(x) \log Q(x)$$

- P(x) ：真实分布中样本属于类别：真实分布中样本属于类别 x 的概率。
- Q(x) ：模型预测样本属于类别：模型预测样本属于类别 x 的概率。  



在分类任务中，真实分布 $$ P $$ 通常为 **one-hot编码**（仅正确类别概率为1），因此交叉熵可简化为：
$$ H(P, Q) = -\log Q(y_{\text{true}}) $$
其中 $$ y_{\text{true}} $$ 是真实类别对应的索引。

---

1. **二分类问题（二元交叉熵）**

$$ \text{Loss} = -\left[ y \log(p) + (1-y) \log(1-p) \right] $$
其中 $$ y \in \{0, 1\} $$ 是真实标签，$$ p $$ 是模型预测正类的概率。

---

**2. 多分类问题（分类交叉熵）**

$$ \text{Loss} = -\sum_{i=1}^C y_i \log(p_i) $$
其中：
- $$ C $$ 是类别总数。
- $$ y_i \in \{0, 1\} $$ 是真实标签是否为第 $$ i $$ 类（one-hot编码）。
- $$ p_i $$ 是模型预测样本属于第 $$ i $$ 类的概率。

---

**3. 实际计算示例**

假设一个三分类任务：
- 真实标签 $$ y = [1, 0, 0] $$（one-hot编码）。
- 模型预测概率 $$ p = [0.7, 0.2, 0.1] $$。

交叉熵损失为：
$$ \text{Loss} = -\left(1 \cdot \log(0.7) + 0 \cdot \log(0.2) + 0 \cdot \log(0.1)\right) = -\log(0.7) \approx 0.3567 $$

---

**4. 与KL散度的关系**

交叉熵与KL散度（Kullback-Leibler Divergence）密切相关：
$$ H(P, Q) = H(P) + D_{\text{KL}}(P \parallel Q) $$
其中：
- $$ H(P) $$ 是真实分布的熵，固定值。
- 最小化交叉熵等价于最小化KL散度，即让预测分布逼近真实分布。

---

5. **注意事项**

1. **数值稳定性**：计算 $$\log$$ 时可能遇到接近0的值，导致数值下溢。实际中可添加微小常数（如 $$\epsilon = 1e-7$$）：
   $$    \text{Loss} = -\sum_{i=1}^C y_i \log(p_i + \epsilon)    $$

2. **概率归一化**：预测值 $$ p_i $$ 需满足概率公理（和为1），通常通过Softmax函数实现：
   $$    p_i = \frac{e^{z_i}}{\sum_{j=1}^C e^{z_j}}    $$