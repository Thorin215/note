# GROUNDHOG: Grounding Large Language Models to Holistic Segmentation



这篇文章介绍了一种名为**Groundhog**的多模态大语言模型（MLLM），旨在通过将大语言模型（LLM）与**整体分割**（holistic segmentation）相结合，实现更细粒度的视觉-语言对齐。文章的主要贡献包括提出了一个新的模型架构、一个大规模的多模态多粒度视觉指令调优数据集（M3G2），以及一系列实验验证了模型在多种视觉-语言任务中的优越性能。

### 1. **背景与动机**
现有的多模态大语言模型（MLLM）通常通过因果语言建模来学习语言到对象的对齐，其中对象通过边界框（bounding box）表示。然而，边界框缺乏像素级的表示，无法捕捉到细粒度的视觉信息，如无定形物体、语义部分、不规则形状的区域等。这导致模型在检测时容易产生歧义，并且在模型出现幻觉（hallucination）时难以诊断问题的根源。

为了解决这些问题，作者提出了**Groundhog**模型，通过将大语言模型与整体分割相结合，实现了像素级的视觉-语言对齐。Groundhog模型能够将文本中的可接地短语（groundable phrases）与图像中的分割掩码（segmentation masks）进行关联，从而提供更细粒度的视觉理解。

### 2. **模型架构**
Groundhog模型的核心思想是将语言接地任务分解为两个部分：**定位**（localization）和**识别**（recognition）。具体来说，模型首先通过一个掩码生成模型（如Mask2Former+）生成一组类无关的实体掩码（entity masks），然后通过掩码特征提取器将这些掩码转换为视觉实体标记（visual entity tokens），输入到MLLM中。MLLM通过检索和合并这些实体掩码，将可接地短语与统一的分割掩码进行关联。

模型的创新点包括：
- **掩码特征提取器**：通过卷积掩码池化层从预训练的视觉基础模型（如CLIP和DINOv2）中提取特征，并将这些特征转换为视觉实体标记。
- **空间提示支持**：模型能够处理用户提供的空间提示（如通过Segment Anything Model生成的掩码），并将其转换为视觉实体特征。
- **语言接地机制**：通过引入一对接地标记（<GRD>和</GRD>），模型能够将可接地短语与视觉实体进行关联，并通过最大像素合并生成最终的分割掩码。

### 3. **数据集：M3G2**
为了训练Groundhog模型，作者构建了一个名为**M3G2**的大规模多模态多粒度视觉指令调优数据集。该数据集包含250万文本-图像对，涵盖了36个子任务，源自27个现有的数据集。M3G2数据集分为四类任务：
1. **接地图像描述（GIC）**：生成与图像中视觉实体相关的描述。
2. **指代表达分割（RES）**：根据给定的指代表达生成分割掩码。
3. **接地视觉问答（GVQA）**：回答与图像相关的问题，并将答案与分割掩码进行关联。
4. **指代对话（RD）**：在用户提供空间提示的情况下进行多模态对话。

### 4. **实验与结果**
作者通过一系列实验验证了Groundhog模型在多种视觉-语言任务中的性能，包括指代表达分割、接地图像描述、接地视觉问答和指代对话。实验结果表明，Groundhog在无需任务特定微调的情况下，能够在多个任务上达到或超越现有模型的性能。此外，Groundhog在减少对象幻觉（object hallucination）方面表现出色，并且在失败案例中提供了易于理解的诊断。

### 5. **贡献与创新**
- **像素级视觉-语言对齐**：Groundhog通过整体分割实现了像素级的视觉-语言对齐，提供了比传统边界框更细粒度的视觉理解。
- **多模态多粒度数据集**：M3G2数据集涵盖了多种视觉-语言任务，提供了丰富的训练数据。
- **模型的可解释性**：通过将掩码生成与语言接地解耦，Groundhog在失败案例中提供了更高的可解释性和诊断能力。

### 6. **局限性与未来工作**
- **数据集质量**：M3G2数据集的质量依赖于现有的学术数据集，未来可以通过数据过滤和扩展来提高数据集的覆盖范围和质量。
- **单图像处理**：当前模型仅处理单张图像，未来可以扩展到3D或视频等多模态数据。

### 7. **总结**
Groundhog模型通过将大语言模型与整体分割相结合，实现了像素级的视觉-语言对齐，提供了更细粒度的视觉理解和更高的可解释性。M3G2数据集为模型的训练提供了丰富的多模态多粒度数据，实验结果表明Groundhog在多种视觉-语言任务中表现出色。未来的工作可以进一步扩展模型的适用范围，并提高数据集的覆盖范围和质量。

![image-20250302002951907](./GROUNDHOG.assets/image-20250302002951907.png)