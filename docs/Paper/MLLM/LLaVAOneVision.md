# LLaVA-OneVision

## 视觉表征

- 采用了`AnyRes`

<img src="https://blog-pic-thorin.oss-cn-hangzhou.aliyuncs.com/1737463264656.png" alt="1737463264656" style="zoom:50%;" />

<img src="https://blog-pic-thorin.oss-cn-hangzhou.aliyuncs.com/1737463339032.png" alt="1737463339032" style="zoom:50%;" />

- LLaVA-NeXT-Interleave是首次尝试报告在所有三种场景下的良好性能，LLaVA-OneVision继承了它的训练配置和数据，以提高性能



## Arch

![1737465768912](https://blog-pic-thorin.oss-cn-hangzhou.aliyuncs.com/1737465768912.png)

1. 语言模型（LLM）：Qwen-2

- **选择**：Qwen-2 作为语言模型（LLM），由参数 ϕ 参数化，表示为 \( f_\phi(\cdot) \)。
- **原因**：
  - **模型规模灵活**：Qwen-2 提供了多种模型大小（如 0.5B、7B、72B），能够适应不同的计算资源和性能需求。
  - **强大的语言能力**：Qwen-2 在公开的检查点中表现出色，具有强大的语言理解和生成能力。
- **创新点**：
  - **开源与灵活性**：Qwen-2 是一个开源模型，允许研究人员根据需求选择不同规模的模型。
  - **多语言支持**：Qwen-2 支持多种语言，增强了模型在多语言任务中的表现。

---

2. 视觉编码器（Vision Encoder）：SigLIP

- **选择**：SigLIP 作为视觉编码器，由参数 ψ 参数化，表示为 $ g_\psi(\cdot) $。
- **功能**：将输入图像 $X_v $ 编码为视觉特征 $ Z_v = g(X_v) $。
- **创新点**：
  - **SigLIP 的优势**：SigLIP 是一种高效的视觉编码器，能够处理高分辨率图像，并在视觉特征提取任务中表现出色。
  - **网格特征的使用**：在实验中，模型不仅使用了最后一层 Transformer 层的输出特征，还考虑了**最后一层前后的网格特征**。这种设计能够捕捉到更丰富的视觉信息，尤其是在需要细节理解的任务中（如图表理解、文档分析）。
  - **高分辨率处理**：SigLIP 的设计使其能够有效处理高分辨率图像，这与 LLaVA-OneVision 的 **AnyRes 策略** 相辅相成。

---

3. 投影器（Projector）：2层MLP

- **选择**：使用一个 2 层的多层感知机（MLP）作为投影器，由参数 θ 参数化，表示为 $p_\theta(\cdot) $。
- **功能**：将视觉特征 \( Z_v \) 投影到语言模型的词嵌入空间，生成一系列视觉令牌 $ H_v = p(Z_v) $。
- **创新点**：
  - **轻量级设计**：2 层 MLP 的设计非常轻量，计算效率高，适合大规模训练和推理。
  - **模态对齐**：投影器的作用是将视觉特征与语言模型的词嵌入空间对齐，从而实现视觉和语言模态的融合。这种设计使得模型能够更好地理解视觉输入，并将其与语言指令结合进行推理。
  - **可扩展性**：投影器的设计简单且可扩展，能够适应不同规模的视觉编码器和语言模型。

---

4. 整体架构的创新点

- **模态融合**：LLaVA-OneVision 通过 **视觉编码器 + 投影器 + 语言模型** 的设计，实现了视觉和语言模态的高效融合。这种设计使得模型能够同时处理视觉输入（如图像、视频）和语言指令，并在多模态任务中表现出色。
- **模块化设计**：每个组件（LLM、视觉编码器、投影器）都是模块化的，可以根据需求替换或升级。例如，视觉编码器可以替换为其他高效的视觉模型（如 CLIP），语言模型可以替换为其他开源 LLM（如 LLaMA）。
- **高效性与性能平衡**：通过选择高效的视觉编码器（SigLIP）和轻量级的投影器（2层MLP），LLaVA-OneVision 在保持高性能的同时，显著降低了计算成本。

## Contribution

- 用于处理高分辨率图像的AnyRes，扩展高质量的指令数据，以及利用当时可用的最佳开放LLM

